# -*- coding: utf-8 -*-
"""risk_binary_class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y9Kh5HyRfi3Q7u3TQ3vvXxSXptY6N7zM
"""

#ZP - BEFORE STARTING - Go up to the options in Colab and select Runtime -> change runtime type -> hardware accelrator type (dropdown) -> (select) GPU. Then click save.

# Install the packages in the Google Colab Runtime. Have to re-do each time you restart I think. Some are probably alread in Colab but I just threw it all in here.
!pip install transformers
!pip install torch
!pip install pandas
!pip install numpy
!pip install sklearn

# -*- coding: utf-8 -*-
"""
@author: B WAL (truncated since I made this public to share with you)
"""

from transformers import pipeline
import torch
import pandas as pd
import os
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments
from transformers import Trainer
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from transformers import AutoTokenizer, DistilBertTokenizerFast, BertForSequenceClassification, DistilBertForSequenceClassification
import matplotlib.pyplot as plt
from torch import nn
from sklearn.utils import compute_class_weight

# Get direcotry for Risk Classified files
risk_data_dir = './Risk Data/'

# Get list of files
risk_files = os.listdir(risk_data_dir)


#ZP - Was testing possible code to get reading files in to a list correct
      #for f in risk_files:
        #print(f+" here")

      #!ls "/content/drive/MyDrive/Risk Data/"

      #files = glob.glob(f"/content/drive/MyDrive/Risk Data/*.csv")
      #for file in files:  
      #  print(test)
        #do_something(file)

# Remove any files that are not .csv
risk_files = [f for f in risk_files if f[-4:] == '.csv']

# Load files into DataFrame 

risk_df = pd.read_csv(risk_data_dir + risk_files[0])

for i in range(1, len(risk_files)):
    print('Appending file {} to dataframe...'.format(i))
    df = pd.read_csv(risk_data_dir + risk_files[i])
    risk_df = risk_df.append(df)

# View dataframe head
risk_df[risk_df['Sentences'].isna()]

# Convert NaN to 0
risk_df['Risk'] = risk_df['Risk'].fillna(0)

# Get label counts
risk_df['Risk'].value_counts()

# Get array of labels
labels = risk_df['Risk'].to_numpy()

# Let's see how a Zero-Shot classification would perform on the data

classifier = pipeline("zero-shot-classification", device=0)

#classifier = pipeline("zero-shot-classification")

# Label to be predicted by classification
candidate_label = ['Risk']

pred_labels = []

#ZP - Testing classifier on batch of first 20.
#ZP - This code has no use in the rest of the notebook. Just note that entire df can be classified in one line rather than a loop. Not sure if it's necessary that much more efficient.
classifier_result = classifier(risk_df['Sentences'].head(1000), candidate_label)
print(classifier_result)
print(len(classifier_result))

#ZP - As of 1/31/2022, added an argument to the the classifier declaration that uses GPU. The bit below should be done in batches to make it faster.
for i in range(risk_df.shape[0]):
    token = risk_df.iloc[i, 0]
    classifier_result = classifier(token, candidate_label)
    if classifier_result['scores'][0] > 0.5:
        pred_labels.append(1)
    else:
        pred_labels.append(0)

# Get accuracy
zeroshot_acc = np.sum(np.array(pred_labels)== labels) / labels.size
print('Zero Shot Classification Accuracy: {}'.format(zeroshot_acc))

# Confusion matrix
zero_cm = confusion_matrix(labels, pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=zero_cm)
disp.plot()

#ZP - disp show was throwing an error in colab so I commented it out
#disp.show()

def split_encode(df, x_col, y_col, tokenizer, val_pct = 0.15, test_pct = 0.15, seed = 2392):
  '''
  Splits data into training, validation and test sets, and then encodes them.
  '''
  train_val, test = train_test_split(df[[x_col, y_col]], test_size=test_pct, random_state=seed, shuffle=True)
  train, val = train_test_split(train_val, test_size = val_pct, random_state=seed, shuffle=True)
  
  

  train_encodings = tokenizer(train[x_col].to_list(), truncation=True, padding=True
                              , max_length=512)
  val_encodings = tokenizer(val[x_col].to_list(), truncation=True, padding=True
                            , max_length=512)
  test_encodings = tokenizer(test[x_col].to_list(), truncation=True, padding=True
                             , max_length=512)

  train_labels = train[y_col].astype(int).to_list()
  val_labels = val[y_col].astype(int).to_list()
  test_labels = test[y_col].astype(int).to_list()  

  return ((train_encodings, train_labels), (val_encodings, val_labels), 
          (test_encodings, test_labels))

# Convert data to Torch dataset
# https://huggingface.co/transformers/v3.1.0/custom_datasets.html

class RiskDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Compute the class weights, which will be used to rebalance the data
# This will be feed into the compute loss function into the CustomTrainer subclass
class_wts = compute_class_weight(class_weight = 'balanced', 
                                 classes = np.unique(labels), 
                                 y = labels)

# Implement a custom trainer to use a weighted loss to account for 
# unbalananced data

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_wts).float()).to(device='cuda')
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

# Get dataset tuples using Distilbert Tokenizer
# https://huggingface.co/distilbert-base-uncased
distil_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
dat_tuple = split_encode(risk_df, 'Sentences', 'Risk', distil_tokenizer)

# Convert data to torch dataset

train_dataset = RiskDataset(dat_tuple[0][0], dat_tuple[0][1])
val_dataset = RiskDataset(dat_tuple[1][0], dat_tuple[1][1])
test_dataset = RiskDataset(dat_tuple[2][0], dat_tuple[2][1])

test_labels = dat_tuple[2][1]


# Set up model


model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

# Reduced batch size to ensure enough memory on the GPU in GoogleCollab
training_args = TrainingArguments("test_trainer")


trainer = CustomTrainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)

trainer.train()

# Predict training data set
# https://huggingface.co/course/chapter3/3?fw=pt

pred = trainer.predict(test_dataset)
preds = np.argmax(pred.predictions, axis=-1)

test_acc = np.sum(preds == test_labels) / len(test_labels)

print('Test Accuracy: {}'.format(test_acc*100))

# Confusion matrix

cm = confusion_matrix(test_labels, preds)
disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm)
disp_cm.plot()
#disp_cm.show()

# Delete model and trainer, and clear cache to save memory
del model
del trainer
torch.cuda.empty_cache()

# https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english
from transformers import AutoTokenizer, AutoModelForSequenceClassification

sst_tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

dat_tuple = split_encode(risk_df, 'Sentences', 'Risk', sst_tokenizer)

# Convert data to torch dataset

train_dataset = RiskDataset(dat_tuple[0][0], dat_tuple[0][1])
val_dataset = RiskDataset(dat_tuple[1][0], dat_tuple[1][1])
test_dataset = RiskDataset(dat_tuple[2][0], dat_tuple[2][1])

test_labels = dat_tuple[2][1]

model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', num_labels=2)

training_args = TrainingArguments("sst_train")

trainer = CustomTrainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)

trainer.train()

# Predict training data set
# https://huggingface.co/course/chapter3/3?fw=pt

pred = trainer.predict(test_dataset)
preds = np.argmax(pred.predictions, axis=-1)

test_acc = np.sum(preds == test_labels) / len(test_labels)

print('Test Accuracy: {}'.format(test_acc*100))

# Confusion matrix

cm = confusion_matrix(test_labels, preds)
disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm)
disp_cm.plot()

# Delete model and trainer, and clear cache to save memory
del model
del trainer
torch.cuda.empty_cache()

# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment

twit_tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")

dat_tuple = split_encode(risk_df, 'Sentences', 'Risk', twit_tokenizer)

# Convert data to torch dataset

train_dataset = RiskDataset(dat_tuple[0][0], dat_tuple[0][1])
val_dataset = RiskDataset(dat_tuple[1][0], dat_tuple[1][1])
test_dataset = RiskDataset(dat_tuple[2][0], dat_tuple[2][1])

test_labels = dat_tuple[2][1]

model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment", num_labels=2, ignore_mismatched_sizes=True)

training_args = TrainingArguments("twitter_train")

trainer = CustomTrainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)

trainer.train()

# Predict training data set
# https://huggingface.co/course/chapter3/3?fw=pt

pred = trainer.predict(test_dataset)
preds = np.argmax(pred.predictions, axis=-1)

test_acc = np.sum(preds == test_labels) / len(test_labels)

print('Test Accuracy: {}'.format(test_acc*100))

# Confusion matrix

cm = confusion_matrix(test_labels, preds)
disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm)
disp_cm.plot()
